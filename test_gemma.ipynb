{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma 2B-IT Testing on RTX 2060\n",
    "\n",
    "This Jupyter Notebook contains code to test the performance and capabilities of the **Gemma 2B-IT model** on an **NVIDIA RTX 2060 GPU**. \n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "* Benchmark model inference speed.\n",
    "* Evaluate memory consumption.\n",
    "* Explore potential applications on this hardware configuration.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "* NVIDIA RTX 2060 or compatible GPU\n",
    "* Jupyter Notebook environment\n",
    "* Necessary libraries (TensorFlow/PyTorch, etc.)\n",
    "\n",
    "**Procedure:**\n",
    "\n",
    "The notebook will guide you through the following steps:\n",
    "\n",
    "1. **Environment Setup:** Installation of required libraries and dependencies.\n",
    "2. **Model Loading:** Loading the pre-trained Gemma 2B-IT model.\n",
    "3. **Data Preparation:** Loading and preprocessing sample data for testing.\n",
    "4. **Inference and Benchmarking:** Running the model on the test data and measuring inference time.\n",
    "5. **Memory Usage Analysis:** Monitoring GPU memory consumption during model execution.\n",
    "\n",
    "**Results:**\n",
    "\n",
    "The notebook will present the findings of the tests, including:\n",
    "\n",
    "* Inference speed on various batch sizes.\n",
    "* GPU memory utilization.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "A summary of the Gemma 2B-IT's performance on the RTX 2060, along with potential use cases and limitations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation \n",
    "\n",
    "Installing the following libraries :\n",
    "```shell\n",
    "pip install -U transformers\n",
    "pip install torch\n",
    "pip uninstall huggingface_hub\n",
    "pip install git+https://github.com/huggingface/huggingface_hub.\n",
    "pip install ipywidgets\n",
    "```\n",
    "\n",
    "### Login \n",
    "To log into hugging face I had to installed a library, and then use it. \n",
    "```shell\n",
    "pip install -U \"huggingface_hub[cli]\"\n",
    "```\n",
    "Then run the following command \n",
    "```shell\n",
    "huggingface-cli login\n",
    "```\n",
    "\n",
    "here is the result of the execution and the successful login:\n",
    "```shell\n",
    "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
    "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
    "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
    "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
    "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
    "\n",
    "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
    "Token can be pasted using 'Right-Click'.\n",
    "Enter your token (input will not be visible):\n",
    "Add token as git credential? (Y/n) Y\n",
    "Token is valid (permission: fineGrained).\n",
    "Your token has been saved in your configured git credential helpers (manager).\n",
    "Your token has been saved to C:\\Users\\David\\.cache\\huggingface\\token\n",
    "Login successful\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U transformers\n",
    "#pip install torch\n",
    "#pip uninstall huggingface_hub\n",
    "#pip install git+https://github.com/huggingface/huggingface_hub.\n",
    "#pip install ipywidgets\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import time\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bebeff85dcc4fc9919d91f143f7869e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "Device name: NVIDIA GeForce RTX 2060\n",
      "Elapsed time: 79.951386 seconds\n",
      "<bos>Tell me a joke about the moon.\n",
      "\n",
      "What do you call a moon that's always crying?\n",
      "\n",
      "A mooner.<eos>\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2b-it\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available\")\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))  # Print GPU name\n",
    "else:\n",
    "    print(\"GPU is not available\")\n",
    "    \n",
    "input_text = \"Tell me a joke about the moon\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "outputs = model.generate(**input_ids,max_new_tokens=128)\n",
    "end_time = time.perf_counter()\n",
    "print(f\"Elapsed time: {end_time - start_time:.6f} seconds\")\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quantized Versions through bitsandbytes -> Quantized Versions through**\n",
    "\n",
    "* 8 bits precision model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784b5b51309b4e9b97cc8e32fe3609ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "Device name: NVIDIA GeForce RTX 2060\n",
      "Elapsed time: 10.703927 seconds\n",
      "<bos>Tell me a joke about the moon.\n",
      "\n",
      "What do you call a moon that's always crying?\n",
      "\n",
      "A moon weep.<eos>\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2b-it\",\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available\")\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))  # Print GPU name\n",
    "else:\n",
    "    print(\"GPU is not available\")\n",
    "    \n",
    "input_text = \"Tell me a joke about the moon\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "outputs = model.generate(**input_ids,max_new_tokens=128)\n",
    "end_time = time.perf_counter()\n",
    "print(f\"Elapsed time: {end_time - start_time:.6f} seconds\")\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quantized Versions through bitsandbytes -> Quantized Versions through**\n",
    "\n",
    "* 4 bits precision model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c3034fabd1b414c9a285d97319a9d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "Device name: NVIDIA GeForce RTX 2060\n",
      "Elapsed time: 5.469612 seconds\n",
      "<bos>Tell me a joke about the moon.\n",
      "\n",
      "What do you call a moon that's always crying?\n",
      "\n",
      "A sad moon!<eos>\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2b-it\",\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available\")\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))  # Print GPU name\n",
    "else:\n",
    "    print(\"GPU is not available\")\n",
    "    \n",
    "input_text = \"Tell me a joke about the moon\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "outputs = model.generate(**input_ids,max_new_tokens=128)\n",
    "end_time = time.perf_counter()\n",
    "print(f\"Elapsed time: {end_time - start_time:.6f} seconds\")\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This experiment explored the impact of quantization on the Gemma 2B-IT model's performance, specifically focusing on inference time and output quality. The table below summarizes the findings:\n",
    "\n",
    "| Number of Bits | Inference Time (seconds) | Output        |\n",
    "|----------------|--------------------------|---------------|\n",
    "| 16             | 79.95                   | A mooner.     |\n",
    "| 8              | 10.7                    | A moon weep   |\n",
    "| 4              | 5.47                    | A sad moon    |\n",
    "\n",
    "As evident from the results, reducing the precision from 16 bits to lower bit widths (8 and 4 bits) significantly decreases inference time. This speed improvement comes at the cost of slight variations in the output. \n",
    "\n",
    "While 16-bit precision yields the most accurate output (\"A mooner.\"), the 8-bit and 4-bit versions produce outputs that are still semantically similar, albeit with minor differences in wording. \n",
    "\n",
    "**Key Takeaway:**\n",
    "\n",
    "For tasks where speed is prioritized over absolute precision, employing lower bit quantization (e.g., 8-bit or 4-bit) with Gemma 2B-IT presents a compelling trade-off. This approach can be particularly beneficial in real-time applications or scenarios with limited computational resources. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
